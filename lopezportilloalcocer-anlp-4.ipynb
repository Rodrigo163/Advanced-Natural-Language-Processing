{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANLP 2019 - Assignment 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Rodrigo Lopez Portillo Alcocer, 805606* (enter your name/student id number here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">Due: Wednesday, December 18, 4pm</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "**NOTE**<br><br>\n",
    "\n",
    "Please first fill in your name and id number at the top of the assignment, and **rename** the assignment file to **yourlastname-anlp-4.ipynb**<br><br>\n",
    "Problems and questions are given in blue boxes like this one. All grey and white boxes must be filled by you (they either require code or a (brief!) discussion). <br><br>\n",
    "Please hand in your assignment by the deadline via Moodle. In case of questions, you can contact the TAs or David via the usual channels.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "In this assignment, you will implement a feedforward neural network and train it with backpropagation to classify intent from the provided dataset (<https://github.com/Dark-Sied/Intent_Classification>). For the purpose of understanding the learning process, the whole dataset is used as both training and test data. (What does that mean for your results?)<br><br>\n",
    "\n",
    "You should implement all part of this exercise using only python + standard library + NumPy. (That is, no specialised machine learning libraries are allowed.) Here is a list of NumPy functions that may or may not be useful for this task: <br>\n",
    "`np.array(), np.eye(), np.reshape(), np.ones(), np.zeros(), np.dot(), np.concatenate(), np.maximum(), np.argmax(), np.sum(), np.uniform()`. <br><br>\n",
    "\n",
    "A more comprehensive introduction to NumPy can be found here: <https://sites.engineering.ucsb.edu/~shell/che210d/numpy.pdf> .\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For your convenience, a function for reading in the dataset:\n",
    "import csv\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "def load_dataset(filename):\n",
    "    intent = []\n",
    "    unique_intent = []\n",
    "    sentences = []\n",
    "    with open(filename, \"r\", encoding=\"latin1\") as f:\n",
    "        data = csv.reader(f, delimiter=\",\")\n",
    "        for row in data:\n",
    "            sentences.append(row[0])\n",
    "            intent.append(row[1])\n",
    "    unique_intent = set(intent)\n",
    "    return sentences, intent, unique_intent\n",
    "            \n",
    "sentences, intent, unique_intent = load_dataset(\"dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting to know the dataset\n",
    "sentences[:5];\n",
    "intent[:5];\n",
    "unique_intent;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Bag-of-Words Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "The first thing you're being asked to do is to convert the text into a bag-of-words representation matrix where the dimension of the matrix is $V$ x $M$ ($M$: number of examples, $V$: vocabulary size) and the label to a matrix of dimension $K$ x $M$ where $K$ is number of classes.   \n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Student solution here.\n",
    "\n",
    "#getting Vocab, V, M Labels and K\n",
    "# assuming we are allowed to use a tokenizer we will tokenize the lowered version of each sentence\n",
    "#and add its unique words.\n",
    "\n",
    "Voc = np.array([])\n",
    "for sent in np.unique(sentences):\n",
    "    sent_words = np.unique(nltk.tokenize.word_tokenize(sent.lower()))\n",
    "    Voc = np.append(Voc, sent_words)\n",
    "    Voc = np.unique(Voc)\n",
    "Lab = np.array(list(unique_intent))\n",
    "V = len(Voc)\n",
    "M = len(sentences)\n",
    "K = len(unique_intent)\n",
    "\n",
    "# B = bag of words repr matrix, L = matrix with labels\n",
    "B = np.zeros((V, M))\n",
    "L = np.zeros((K, M))\n",
    "\n",
    "# filling B \n",
    "for j in range(M):\n",
    "    # and L, assuming every sentence has only one label\n",
    "    i2 = np.where(Lab == intent[j])[0][0]\n",
    "    L[i2][j] = 1\n",
    "    for word in nltk.tokenize.word_tokenize(sentences[j].lower()):\n",
    "        i = np.where(Voc == word)[0][0]\n",
    "        B[i][j] += 1\n",
    "\n",
    "#checking that every example has a label\n",
    "# False in [1 in L[:,j] for j in range(M)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "For the classification task, the softmax activation function for the output layer with K classes is given by: \n",
    "$softmax(z_i) = \\frac{e^{z_i}}{{\\sum_{j=1}^{K}e^{z_j}}}$ <br>\n",
    "The activation function of the hidden neurons is a non-linear function. We have seen tanh being used in class, but more common these days are for example ReLU or sigmoid, given by: <br>\n",
    "$ReLU(z)=max(0,z)$ <br>\n",
    "$sigmoid(z)=\\frac{1}{1+e^{-z}}$ <br>\n",
    "\n",
    "Implement the softmax, ReLU, and sigmoid activation function in such a way that it accepts NumPy array and matrices. Plot the ReLU and sigmoid functions, as well as their derivatives. Observe the plot and discuss briefly what the advantages and disadvantages of the ReLU and sigmoid activation function might be. \n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Student solution here.\n",
    "\n",
    "def ReLU(M):\n",
    "    if M.ndim >1: #matrix case\n",
    "        #applies ReLU to every element of the input matrix\n",
    "        #returns a matrix will \"Rellued\" elements\n",
    "        #making sure that M is a numpy array\n",
    "        M = np.asarray(M)\n",
    "        m, n = M.shape\n",
    "        sol = np.zeros((m, n))\n",
    "        for i in range(m):\n",
    "            for j in range(n):\n",
    "                sol[i][j] = np.max([0,M[i][j]])\n",
    "    else:  \n",
    "        sol = np.array([np.max([0,x]) for x in M])\n",
    "    return sol\n",
    "\n",
    "def sigmoid(M):\n",
    "    #same as above but diff function\n",
    "    if M.ndim>1:\n",
    "        M = np.asarray(M)\n",
    "        m, n = M.shape\n",
    "        sol = np.zeros((m, n))\n",
    "        for i in range(m):\n",
    "            for j in range(n):\n",
    "                sol[i][j] = 1/(1+np.exp(M[i][j]))\n",
    "    else:\n",
    "        sol = np.array([np.exp(-x) for x in M])\n",
    "        sol = np.array([1/(1+x) for x in sol])\n",
    "    return sol\n",
    "\n",
    "def softmax(ar):\n",
    "    sol = np.array([np.exp(x) for x in ar])\n",
    "    sol /= np.sum(sol)\n",
    "    return sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plotting\n",
    "#ReLU\n",
    "x_re = np.linspace(-100, 100, 10000)\n",
    "y_re = ReLU(x_re)\n",
    "plt.subplot(221)\n",
    "plt.plot(x_re, y_re)\n",
    "plt.title('ReLU plot')\n",
    "\n",
    "#ReLU derivative\n",
    "re_grad = np.gradient(y_re)\n",
    "plt.subplot(223)\n",
    "plt.plot(x_re, re_grad)\n",
    "plt.title('ReLU gradient plot')\n",
    "\n",
    "#Sigmoid\n",
    "x_sig = np.linspace(-5, 5, 100)\n",
    "y_sig = sigmoid(x_sig)\n",
    "plt.subplot(222)\n",
    "plt.plot(x_sig, y_sig)\n",
    "plt.title('Sigmoid plot')\n",
    "\n",
    "#Sigmoid derivative\n",
    "sig_grad = np.gradient(y_sig)\n",
    "plt.subplot(224)\n",
    "plt.plot(x_sig, sig_grad)\n",
    "plt.title('Sigmoid gradient plot')\n",
    "\n",
    "plt.subplots_adjust(wspace = 0.5, hspace = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By seeing the plots I would be inclined to say that using ReLU will result in better speed performance since we only need a boolean operation to deliver an output, while calculating sigmoid and its derivative will be computationally more expensive. Using ReLU also allow us to forget about the possible value explotions caused by exponential functions. \n",
    "\n",
    "One possible disadvantage could of using ReLU is that for a very wide range of values the activation might not be accurate/representative of the entries diversity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Feedforward Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Now that you have created the input matrix, we can implement our neural network and perform a forward propagation to classify intent. To perform the forward propagation, you should compute $z^{l}$ and pass it through the activation function for each layer, given by: <br><br>\n",
    "$z^{l} = W^{l}a^{l-1} + b^{l}$ <br>\n",
    "$a^{l} = g(z^{l})$ <br>\n",
    "where $W^{l}$ is a weight matrix between layer $l$ and $l+1$, $z^{l}$ is value of the hidden layer at layer $l$ before activation, $a^{l}$ is value of the hidden layer at layer $l$ after activation, and $b^{l}$ is bias term for layer $l$.\n",
    "\n",
    "You should implement the feedforward computation that computes $\\hat{y_{i}}$ for every example $i$. The neural network has 3 layers - an input layer, a hidden layer and an output layer, where the hidden layer has 150 neurons. Don't forget to include the bias term. Use ReLU as the activation function for the hidden layer and softmax for the output layer. For parameters initialization, use random values from uniform distribution in the range (-1,1). Provide a seed value to the random number generator, to make the results reproducible. The purpose of using this kind of initialisation is to break symmetry and ensure that different neurons can learn different non-linear functions. (Hint: use vectorization methods instead of a for loop for speedup.) <br><br>\n",
    "\n",
    "Use this neural network to predict the intent and calculate the accuracy of the classifier. (Should you be expecting high numbers yet?)\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Student solution here.\n",
    "np.random.seed(27)\n",
    "#W2 = second weight matrix of size num_classes=K x d=150 By default random values from uniform distribution in the range (-1,1)\n",
    "#W1 = first weight matrix of size d=150 x V=voc size. By default random values from uniform distribution in the range (-1,1)\n",
    "#b2 = first set of biases 150 x 1\n",
    "W1 = np.random.uniform(-1, 1, (150, V))\n",
    "b2 = np.random.uniform(-1, 1, 150)\n",
    "W2 = np.random.uniform(-1, 1, (K, 150))\n",
    "bL = np.random.uniform(-1, 1, K)\n",
    "\n",
    "#FfNN = Feedforward Neural Network as described above. Only one hidden layer with 150 neurons by default, first applying ReLU between input and hidden, then softmax between hidden and output\n",
    "def FfNN(Input, Lab, W1, b2, W2, bL, d=150, probs = 1):\n",
    "    #Input = Input layer of length n\n",
    "    #Hidden = hidden layer of length d x 1\n",
    "    #L list with output labels\n",
    "    #Output = output layer of length num_classes\n",
    "    #if probs = 1 then the output will consist of the respective probabilities\n",
    "    # if probs = 0 the output will be the canonical vector with a 1 in the argmax out the probs/\n",
    "    np.random.seed(27)\n",
    "    n = len(Input)\n",
    "\n",
    "    \n",
    "    Hidden = ReLU(np.matmul(W1, Input) + b2)\n",
    "    num_classes = len(Lab)\n",
    "    W2 = np.random.uniform(-1, 1, (num_classes, d))\n",
    "    bL = np.random.uniform(-1, 1, num_classes)\n",
    "    \n",
    "    Output = softmax(np.matmul(W2, Hidden) + bL)\n",
    "    if probs == 1:   \n",
    "        return Output\n",
    "    elif probs == 0:\n",
    "        Output = (Output == np.max(Output)).astype(int)\n",
    "        return Output\n",
    "    \n",
    "    \n",
    "def FfNN_mat(Input_matrix, Lab, W1, b2, W2, bL, probs = 1):\n",
    "    m, n = Input_matrix.shape\n",
    "    L = len(Lab)\n",
    "    sol = np.zeros((L, n))\n",
    "    for j in range(n):\n",
    "        sol[:,j] = FfNN(Input_matrix[:,j], Lab,W1, b2, W2, bL, 150, probs)\n",
    "    return sol\n",
    "\n",
    "#computing for out B input matrix\n",
    "#reminder, column j represents the probabilities for the j-th example.\n",
    "# output_matrix_probs[i][j] = prob of j-th example belonging to the ith category\n",
    "output_matrix_probs =  FfNN_mat(B, Lab, W1, b2, W2, bL, 1)\n",
    "\n",
    "#in order to get accuracy we need to compare one hot vectors\n",
    "output_matrix_hots = FfNN_mat(B, Lab, W1, b2, W2, bL, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04582210242587601"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#getting accuracy by comparing our results to the previously generated L matrix\n",
    "#if there is no false in L[:,j] == output_matrix_hots[:,j] then both arrays are identical\n",
    "#which means success\n",
    "results = np.array([not False in (L[:,j] == output_matrix_hots[:,j]) for j in range(output_matrix_hots.shape[1])])\n",
    "accuracy = np.sum(results)/len(results)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not surprising that we get a terrible accuracy since our parameters where initialized at random and no learning step as yet been implemented. If we had gotten a good accuracy I would be omw to Las Vegas :P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "You will now implement the backpropagation algorithm to compute the gradient of the cost function with respect to the neural network weights' and bias term.  First of all, implement the cross entropy loss function to monitor if your model is actually learning. Remember that in backpropagation, we want to propagate the error signal to measure how much each neuron in the hidden layer contributes to the error in the output layer. It is more or less similar to forward propagation but in a reverse direction. For the output layer, set $\\delta$ for cross entropy loss: <br><br>\n",
    "$\\delta^{L}= \\hat{y} - y$ <br> where $L$ is the output layer and $\\hat{y}$ is prediction of $y$. <br>\n",
    "\n",
    "For the remaining hidden layer $l$, set: <br><br>\n",
    "$\\delta^{l} = (W^{l})^{T}\\delta^{l+1} \\odot g'(z^{l})$ <br> where $\\odot$ is an element-wise product of matrices (Hadamard product), $g'$ is the derivative of the activation function. <br>\n",
    "\n",
    "The derivative of the ReLU is given by:  $ReLU'(z) = \\begin{cases} 1 & \\text{if } z > 0 \\\\\n",
    "                                                                                                                                      0 & \\text{otherwise}.\\end{cases}$<br>\n",
    "\n",
    "By calculating the error term for each layer, you can then use the error term to calculate the partial derivatives $\\frac{\\partial \\mathcal{L}}{\\partial W^{l}} = \\delta_{l+1} (a^{l})^{T}$ and $\\frac{\\partial \\mathcal{L}}{\\partial b^{l}} = \\delta_{l+1}$ and perform batch gradient descent to update the parameter. (Batch gradient descent = run through all training instances and compute the gradient, then make the weight update.) Make sure that you accumulate the gradients for all the training samples and divide it by number of samples before doing the update. <br><br>\n",
    "\n",
    "Here is some simple pseudocode to help with the training procedure: <br>\n",
    " * for number of epoch:\n",
    "    > define gradient accumulator $\\Delta w=0, \\Delta b=0$ for each weight and bias term <br>\n",
    "    > define cost accumulator $\\Delta \\mathcal{L}=0$ for the loss <br>\n",
    "    \n",
    "    > for each training example $i$:<br>\n",
    "        >> perform forward propagation <br>\n",
    "        >> calculate loss on example $i, L_{i}$ <br>\n",
    "        >> $\\Delta \\mathcal{L} = \\Delta \\mathcal{L} + L_{i}$ <br> <br> \n",
    "        >> perform backpropagation <br>\n",
    "        >> $\\Delta w = \\Delta w + \\frac{\\partial \\mathcal{L}}{\\partial W}$ for each weight <br>\n",
    "        >> $\\Delta b = \\Delta b + \\frac{\\partial \\mathcal{L}}{\\partial b}$ for each bias term <br> \n",
    "        \n",
    "    > calculate the cost, which is just average loss ($Cost = \\frac{1}{m}\\Delta \\mathcal{L}$) <br>\n",
    "    > $w = w - \\frac{\\alpha}{m}\\Delta w$ for each weight <br>\n",
    "    > $b = b - \\frac{\\alpha}{m}\\Delta b$ for each bias term <br> \n",
    "\n",
    "Run the training for 1000 epoch using learning rate = 0.005 and use this neural network to predict the intent and calculate the accuracy of the classifier. (Hint: the dimension of $\\delta^{l}$ should match the dimension of $a^{l}$, and the dimension of $\\frac{\\partial \\mathcal{L}}{\\partial W^{l}}$ and $\\frac{\\partial \\mathcal{L}}{\\partial b^{l}}$ should match the dimension of $W^{l}$ and $b^{l}$, respectively).<br><br>\n",
    "\n",
    "Plot the cost function for each iteration and compare the results after training with results from Problem 3. Discuss what you observe!\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Student solution here.\n",
    "\n",
    "#preparing functions we will use\n",
    "def relu_derivative(M):\n",
    "    if M.ndim >1: #matrix case\n",
    "        #applies ReLU_derivative to every element of the input matrix\n",
    "        #returns a matrix will \"Rellued_der\" elements\n",
    "        #making sure that M is a numpy array\n",
    "        M = np.asarray(M)\n",
    "        m, n = M.shape\n",
    "        sol = np.zeros((m, n))\n",
    "        for i in range(m):\n",
    "            for j in range(n):\n",
    "                if M[i][j] >= 0:\n",
    "                    sol[i][j] = 1\n",
    "    else: #array case\n",
    "        sol = np.array([x>=0 for x in M]).astype(int)\n",
    "    return sol\n",
    "\n",
    "\n",
    "def output_error(prediction, real):\n",
    "    #both prediction and real are 1D arrays\n",
    "    return prediction-real\n",
    "\n",
    "\n",
    "def hidden_error(weights, output_layer_error, hidden_node_values):\n",
    "    p1 = np.matmul(weights.transpose(),output_layer_error)\n",
    "    p2 = relu_derivative(hidden_node_values)\n",
    "    return p1*p2\n",
    "\n",
    "#functions to accumulate weight/bias errors\n",
    "def acc_bias_error(temp_layer_error, bias_index):\n",
    "    return temp_layer_error[bias_index]\n",
    "\n",
    "def acc_weight_error(prec_layer, follow_error_layer, follow_index, prec_index):\n",
    "    return prec_layer[prec_index] * follow_error_layer[follow_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init of all our variables\n",
    "Lab = np.array(list(unique_intent))\n",
    "V = len(Voc)\n",
    "M = len(sentences)\n",
    "K = len(unique_intent)\n",
    "np.random.seed(27)\n",
    "#W2 = second weight matrix of size num_classes=K x d=150 By default random values from uniform distribution in the range (-1,1)\n",
    "#W1 = first weight matrix of size d=150 x V=voc size. By default random values from uniform distribution in the range (-1,1)\n",
    "#b2 = first set of biases 150 x 1\n",
    "W1 = np.random.uniform(-1, 1, (150, V))\n",
    "b2 = np.random.uniform(-1, 1, 150)\n",
    "W2 = np.random.uniform(-1, 1, (K, 150))\n",
    "bL = np.random.uniform(-1, 1, K)\n",
    "n_epochs = 2\n",
    "learning_rate = 0.005\n",
    "costs = []\n",
    "\n",
    "#tranposing already out bag-of-words and label matrices \n",
    "Bt = B.transpose()\n",
    "Lt = L.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-18 15:44:33 -- Done epoch 1 of 2\n",
      "2019-12-18 15:50:42 -- Done epoch 2 of 2\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "for epoch in range(n_epochs):\n",
    "    delta_W1 = np.zeros((150, V))\n",
    "    delta_b2 = np.zeros(150)\n",
    "    delta_W2 = np.zeros((K, 150))\n",
    "    delta_bL = np.zeros(K)\n",
    "    examples_loss = []\n",
    "    \n",
    "    for example_index in range(len(Bt)):\n",
    "        input_vector = Bt[example_index]\n",
    "        \n",
    "        #doing Fforward and getting cross_entr_loss\n",
    "        softmax_output = FfNN(input_vector, Lab, W1, b2, W2, bL)\n",
    "        gold_output = Lt[example_index]\n",
    "        delta_output = output_error(softmax_output, gold_output)\n",
    "        examples_loss.append(delta_output)\n",
    "        \n",
    "        #now getting the error vectors and gradients\n",
    "        z_hidden = np.matmul(W1, input_vector) + b2\n",
    "        delta_hidden = hidden_error(W2, delta_output, z_hidden)\n",
    "        \n",
    "        #filling the delta matrices\n",
    "        for i in range(delta_W1.shape[0]):\n",
    "            for j in range(delta_W1.shape[1]):\n",
    "                delta_W1[i][j] += acc_weight_error(input_vector, delta_hidden, i, j)\n",
    "                \n",
    "        for i in range(delta_W2.shape[0]):\n",
    "            for j in range(delta_W2.shape[1]):\n",
    "                delta_W2[i][j] += acc_weight_error(relu_derivative(z_hidden), delta_output, i, j)\n",
    "        \n",
    "        for i in range(len(delta_b2)):\n",
    "            delta_b2[i] += acc_bias_error(delta_hidden, i)\n",
    "            \n",
    "        for i in range(len(delta_bL)):\n",
    "            delta_bL[i] += acc_bias_error(delta_output, i)\n",
    "            \n",
    "    costs.append(sum(examples_loss) / len(examples_loss))\n",
    "    \n",
    "    #updating weights and biases aka:\n",
    "    W1 = W1 - (learning_rate*delta_W1/M)\n",
    "    W2 = W2 - (learning_rate*delta_W2/M)\n",
    "    b2 = b2 - (learning_rate*delta_b2/M)\n",
    "    bL = bL - (learning_rate*delta_bL/M)\n",
    "    \n",
    "    print('{2} -- Done epoch {0} of {1}'.format(epoch+1, n_epochs, datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of time reasons we only computed two epochs, which resulted in a very poor graph and is not enough to conclude that our network is properly training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Mini-Batch and Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "As a bonus, train the neural network using mini-batch gradient descent with batch size = 64 and stochastic gradient descent (i.e., batch size = 1) for 1000 epoch using learning rate = 0.005. Plot the cost vs iteration for both cases and briefly discuss your observation!  \n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Student solution here.\n",
    "\n",
    "#For the Mini-Batch approach we can copy our previous try and add the sampling step before starting training\n",
    "\n",
    "#MINI-Batches\n",
    "\n",
    "\n",
    "#training\n",
    "for epoch in range(n_epochs):\n",
    "    delta_W1 = np.zeros((150, V))\n",
    "    delta_b2 = np.zeros(150)\n",
    "    delta_W2 = np.zeros((K, 150))\n",
    "    delta_bL = np.zeros(K)\n",
    "    examples_loss = []\n",
    "    mini_indices = np.random.randint(len(Bt), 64) #MINI VERSION\n",
    "    B_mini = Bt[mini_indices, :] #MINI VERSION\n",
    "    for example_index in range(len(B_mini)): #MINI VERSION\n",
    "        input_vector = Bt[example_index]\n",
    "        \n",
    "        #doing Fforward and getting cross_entr_loss\n",
    "        softmax_output = FfNN(input_vector, Lab, W1, b2, W2, bL)\n",
    "        gold_output = Lt[example_index]\n",
    "        delta_output = output_error(softmax_output, gold_output)\n",
    "        examples_loss.append(delta_output)\n",
    "        \n",
    "        #now getting the error vectors and gradients\n",
    "        z_hidden = np.matmul(W1, input_vector) + b2\n",
    "        delta_hidden = hidden_error(W2, delta_output, z_hidden)\n",
    "        \n",
    "        #filling the delta matrices\n",
    "        for i in range(delta_W1.shape[0]):\n",
    "            for j in range(delta_W1.shape[1]):\n",
    "                delta_W1[i][j] += acc_weight_error(input_vector, delta_hidden, i, j)\n",
    "                \n",
    "        for i in range(delta_W2.shape[0]):\n",
    "            for j in range(delta_W2.shape[1]):\n",
    "                delta_W2[i][j] += acc_weight_error(relu_derivative(z_hidden), delta_output, i, j)\n",
    "        \n",
    "        for i in range(len(delta_b2)):\n",
    "            delta_b2[i] += acc_bias_error(delta_hidden, i)\n",
    "            \n",
    "        for i in range(len(delta_bL)):\n",
    "            delta_bL[i] += acc_bias_error(delta_output, i)\n",
    "            \n",
    "    costs.append(sum(examples_loss) / len(examples_loss))\n",
    "    \n",
    "    #updating weights and biases aka:\n",
    "    W1 = W1 - (learning_rate*delta_W1/M)\n",
    "    W2 = W2 - (learning_rate*delta_W2/M)\n",
    "    b2 = b2 - (learning_rate*delta_b2/M)\n",
    "    bL = bL - (learning_rate*delta_bL/M)\n",
    "    \n",
    "    print('{2} -- Done epoch {0} of {1}'.format(epoch+1, n_epochs, datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stocastic Batch version\n",
    "#same as before but now sampling only one row per epoch\n",
    "\n",
    "#training\n",
    "for epoch in range(n_epochs):\n",
    "    delta_W1 = np.zeros((150, V))\n",
    "    delta_b2 = np.zeros(150)\n",
    "    delta_W2 = np.zeros((K, 150))\n",
    "    delta_bL = np.zeros(K)\n",
    "    examples_loss = []\n",
    "    mini_indices = np.random.randint(len(Bt)) #Stocastic VERSION\n",
    "    B_mini = Bt[mini_indices, :] #Stoch VERSION\n",
    "    for example_index in range(len(B_mini)): #Stoch VERSION\n",
    "        input_vector = Bt[example_index]\n",
    "        \n",
    "        #doing Fforward and getting cross_entr_loss\n",
    "        softmax_output = FfNN(input_vector, Lab, W1, b2, W2, bL)\n",
    "        gold_output = Lt[example_index]\n",
    "        delta_output = output_error(softmax_output, gold_output)\n",
    "        examples_loss.append(delta_output)\n",
    "        \n",
    "        #now getting the error vectors and gradients\n",
    "        z_hidden = np.matmul(W1, input_vector) + b2\n",
    "        delta_hidden = hidden_error(W2, delta_output, z_hidden)\n",
    "        \n",
    "        #filling the delta matrices\n",
    "        for i in range(delta_W1.shape[0]):\n",
    "            for j in range(delta_W1.shape[1]):\n",
    "                delta_W1[i][j] += acc_weight_error(input_vector, delta_hidden, i, j)\n",
    "                \n",
    "        for i in range(delta_W2.shape[0]):\n",
    "            for j in range(delta_W2.shape[1]):\n",
    "                delta_W2[i][j] += acc_weight_error(relu_derivative(z_hidden), delta_output, i, j)\n",
    "        \n",
    "        for i in range(len(delta_b2)):\n",
    "            delta_b2[i] += acc_bias_error(delta_hidden, i)\n",
    "            \n",
    "        for i in range(len(delta_bL)):\n",
    "            delta_bL[i] += acc_bias_error(delta_output, i)\n",
    "            \n",
    "    costs.append(sum(examples_loss) / len(examples_loss))\n",
    "    \n",
    "    #updating weights and biases aka:\n",
    "    W1 = W1 - (learning_rate*delta_W1/M)\n",
    "    W2 = W2 - (learning_rate*delta_W2/M)\n",
    "    b2 = b2 - (learning_rate*delta_b2/M)\n",
    "    bL = bL - (learning_rate*delta_bL/M)\n",
    "    \n",
    "    print('{2} -- Done epoch {0} of {1}'.format(epoch+1, n_epochs, datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of time limitations I was not able to try these methods but my guess is that that they will go through all epochs much faster (as they only have to do a small fraction of the previous number of calculations). \n",
    "\n",
    "In the case in which we go through all the examples each epoch will probably result in a smooth minimization of the cost each epoch, while in the sampling methods we will likely find periods for which the cost increazed between epochs but we will also observe a general decreasing trend (hopefully)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "ctrl-q"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANLP 2019 - Assignment 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Rodrigo Lopez Portillo Alcocer, 805606* (enter your name/student id number here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">Due: Wednesday, December 4th (before class)</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "**NOTE**<br><br>\n",
    "\n",
    "Please first fill in your name and id number at the top of the assignment, and **rename** the assignment file to **yourlastname-anlp-3.ipynb**<br><br>\n",
    "Problems and questions are given in blue boxes like this one. All grey and white boxes must be filled by you (they either require code or a (brief!) discussion). <br><br>\n",
    "Please hand in your assignment by the deadline via Moodle. In case of questions, you can contact the TAs or David via the usual channels.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "In this assignment, you will implement a bigram part-of-speech (POS) tagger based on Hidden Markov Models from scratch. Using NLTK is disallowed, except for the modules explicitly listed below. For this, you will need to develop and utilize the following modules:<br>\n",
    "1. Corpus reader and writer<br>\n",
    "2. Training procedure, including smoothing<br>\n",
    "3. Viterbi tagging, including unknown word handling <br>\n",
    "4. Evaluation<br>\n",
    "The task is mostly very straightforward, but each step requires careful design. Thus, we suggest you proceed in the following way.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Viterbi Algorithm [33 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "First, implement the Viterbi algorithm for finding the optimal state (tag) sequence given the sequence of observations (words). <br><br>\n",
    "In order to test your implementation, verify that you compute the correct state sequence for some examples from Eisner's ice cream model (see lecture) for which the solutions are known.<br><br>\n",
    "Demonstrate that your algorithm computes the correct state sequence for ['3','1','3'] as in the lecture.<br><br>\n",
    "Make sure that your algorithm is correct before proceeding to the other tasks! In order to do this, please also test your module with a longer observation sequence. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EISNER_STATES = ['C','H']\n",
    "#EISNER_INITIAL_PROBS = {'C': 0.2, 'H': 0.8}\n",
    "#EISNER_TRANSITIONS = {'C': {'C':0.6, 'H': 0.4}, 'H': {'C':0.3, 'H':0.7}}\n",
    "#EISNER_EMISSIONS = {'C': {'1':0.5,'2':0.4,'3':0.1},'H': {'1':0.2, '2':0.4,'3':0.4}}\n",
    "\n",
    "\n",
    "# your code goes here\n",
    "states1 = ['C','H']\n",
    "pi1 = {'C': 0.2, 'H': 0.8}\n",
    "a1 = {'C': {'C':0.6, 'H': 0.4}, 'H': {'C':0.3, 'H':0.7}}\n",
    "b1 = {'C': {'1':0.5,'2':0.4,'3':0.1},'H': {'1':0.2, '2':0.4,'3':0.4}}\n",
    "unique_words = ['1', '2', '3'] #to handle unk in Eisner case\n",
    "\n",
    "def viterbi(obs, states, pi, a, b):\n",
    "    #obs list of observations\n",
    "    #states list of possible states\n",
    "    #pi dict with initial probs\n",
    "    #a dict with transition probs\n",
    "    #b dict with emission probs\n",
    "    \n",
    "    global unique_words #to check if the current observation is unk\n",
    "    #faster to search for it in unique_words that in whole corpus\n",
    "    np.random.seed(42)\n",
    "    obs = [i.lower() for i in obs]\n",
    "    T = len(states)\n",
    "    N = len(obs)\n",
    "    vit = np.zeros([T, N])  #init both matrices\n",
    "    backp = np.ones([T, N])\n",
    "    #base case from initial prob and first observation\n",
    "    for i in range(T):\n",
    "        #adding how to handle unk \n",
    "        if obs[0] in unique_words:\n",
    "            vit[i,0] = np.exp(np.log(pi[states[i]]) + np.log(b[states[i]][obs[0]]))\n",
    "        else:#unk handling\n",
    "            #random choice of state\n",
    "            rand_state = np.random.choice(states)\n",
    "            vit[i,0] = pi[rand_state]\n",
    "        backp[i,0] = 0\n",
    "    \n",
    "    #recursive step\n",
    "    for j in range(1, N):\n",
    "        for i in range(T):\n",
    "            if obs[j] in unique_words:\n",
    "                temp = [np.exp(np.log(vit[s, j-1]) + np.log(a[states[s]][states[i]]) + np.log(b[states[i]][obs[j]])) for s in range(T)]\n",
    "                vit[i, j] = np.max(temp)\n",
    "            else: #unk handling\n",
    "                temp = [vit[s, j-1] for s in range(T)]\n",
    "                vit[i, j] = np.max(temp)                            \n",
    "            backp[i, j] = int(np.argmax(temp))\n",
    "    \n",
    "    #termination steps\n",
    "    best_path_prob = np.max(vit[:,-1])\n",
    "    best_path_pointer = np.argmax(vit[:,-1])\n",
    "    \n",
    "    #finally, retrieving the best_prob path\n",
    "    path = np.zeros([N]).astype(int)\n",
    "    for i in range(N):\n",
    "        if i == 0:\n",
    "            path[i] = best_path_pointer\n",
    "        else:\n",
    "            path[i] = backp[path[i-1],N-i]\n",
    "    #reversing it to have the correct order\n",
    "    path = path[::-1]\n",
    "    \n",
    "    #from indices [1,0,1,1,1,0] to states [H,C,H,H,H,C] \n",
    "    #\n",
    "    path = [states[path[i]] for i in range(N)]\n",
    "    return path, best_path_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['H', 'H', 'H'], 0.012543999999999996)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs1 = np.random.randint(1,4,20).astype(str)\n",
    "viterbi(obs1, states1, pi1, a1, b1);\n",
    "\n",
    "obs2 = ['3','1', '3']\n",
    "viterbi(obs2, states1, pi1, a1, b1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: HMM Training [33 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Second, learn parameters, i.e. the initial, transition, and emission probabilities, for your HMM from *annotated* data. Implement a maximum likelihood training procedure (with smoothing) for supervised learning of HMMs. We will be using the Wall Street Journal corpus (part of the Penn Treebank). It is a licensed corpus, so please do not redistribute the files. The zip archive provided on Moodle contains a training set, a test set, and an evaluation set. The training set (`wsj_tagged_train.tt`) and the evaluation set (`wsj_tagged_eval.tt`) are written in the commonly used CoNLL format. They are text files with two colums; the first column contains the words, the POS tags are in the second column, and empty lines delimit sentences. The test set (`wsj_tagged_test.t`) is a copy of the evaluation set with tags stripped. You should tag this test set using your tagger and then compare your results with the gold-standard ones in the provided tagged evaluation file. The corpus uses the Penn Treebank tagset.<br><br>\n",
    "You are welcome to use any NLTK data structures from the two modules `nltk.corpus.reader` (and submodules) and `nltk.probability`. The latter includes a number of smoothing procedures, which you may want to apply to your corpus frequency counts. Take care to get NLTK to make the smoothed probability distributions sum to one. Experiment with unsmoothed distributions, Laplace add-one smoothing, and at least one further smoothing procedure.<br><br>\n",
    "Note that your tagger will initially fail to produce output for sentences that contain words you haven't seen in training. If you have such a word $w$ appear at sentence position $t$, you will have $b_j(w) = 0$ for all states/tags $j$, and therefore $V_t(j) = 0$ for all $j$. Adapt your tagger by implementing the following crude approach to unknown words. Whenever you get $V_t(j) = 0$ for all $j$ because of an unknown word at position $t$, pretend that $b_j(w) = 1$ for all $j$. This will basically set $V_t(j) = max_iV_{t-1}(i)a_{ij}$, and allow you to interpolate the missing POS tag based on the transition probabilities alone.<br><br>\n",
    "Note 2: In order to avoid additional problems with zero-probability transitions when applying your model to the test set, make sure that you tag the corpus sentence by sentence (i.e., compute the optimal tag sequence for each sentence independently). \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS_train = \"wsj_tagged/wsj_tagged_train.tt\"\n",
    "CORPUS_gold = \"wsj_tagged/wsj_tagged_eval.tt\"\n",
    "CORPUS_test = \"wsj_tagged/wsj_tagged_test.t\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "corp_train = nltk.corpus.reader.conll.ConllCorpusReader(root = '', fileids=CORPUS_train, columntypes = ['words', 'pos'])\n",
    "corp_gold = nltk.corpus.reader.conll.ConllCorpusReader(root = '', fileids=CORPUS_gold, columntypes = ['words', 'pos'])\n",
    "corp_test = nltk.corpus.reader.conll.ConllCorpusReader(root = '', fileids=CORPUS_test, columntypes = ['words', 'pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the initial state probabilities\n",
    "#we can directly compute the probabilities without having to smooth in this case\n",
    "#if it has an initial probability then it has at least one element.\n",
    "fdist = nltk.probability.FreqDist(tagged_tuple[1] for tagged_tuple in corp_train.tagged_words())\n",
    "total_words = np.sum([fdist[key] for key in fdist.keys()])\n",
    "for key in fdist.keys():\n",
    "    fdist[key] /= total_words\n",
    "#now we have the dict with our initial probabilities\n",
    "pi = fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now to create the transition probs\n",
    "\n",
    "def tagged_sentence_condit_counter(read_corpus):\n",
    "    #counts transitions in the given corpus and returns them in the cfdist form\n",
    "    cfdist = nltk.probability.ConditionalFreqDist()\n",
    "    for tagged_sentence in read_corpus.tagged_sents():\n",
    "        i=0\n",
    "        while i < len(tagged_sentence)-1: #while we still have at least two words\n",
    "            current = tagged_sentence[i][1]\n",
    "            sig = tagged_sentence[i+1][1]\n",
    "            cfdist[current][sig] +=1  #!!! main step for creating the structure\n",
    "            i +=1\n",
    "    return cfdist\n",
    "\n",
    "cfdist_transition = tagged_sentence_condit_counter(corp_train)\n",
    "\n",
    "#smoothing with k=1\n",
    "# we have to keep track of how many counts we add\n",
    "#number of tags squared\n",
    "#----------------------------------------------------\n",
    "#NEVERMIND :P\n",
    "#this counting is very inefficient. It is tempting to get it counting the number of keys in cfdist_transition\n",
    "#but if we had one or more tags that only appeared at the end of the sentences they wouldn't\n",
    "#be represented in the keys of cfdcfdist_transition, but inside one of its sub dicts\n",
    "#because it never appeared in current, only as sig\n",
    "\n",
    "#is there a faster way than this?\n",
    "\n",
    "#counting with np.unique\n",
    "#added_counts = len(np.unique([corp_train.tagged_words()[i][1] for i in range(len(corp_train.tagged_words()))]))**2\n",
    "\n",
    "#trying with base.set\n",
    "#added_counts = len(set([corp_train.tagged_words()[i][1] for i in range(len(corp_train.tagged_words()))]))**2\n",
    "#--------------------------------------------------------\n",
    "#finally the version that worked, using the previous fdist\n",
    "added_counts = len(fdist)**2\n",
    "\n",
    "#this not only smoothes the entries that already existed, but also adds all the other posibilities to our transition matrix (dict in this case)\n",
    "for tag1 in cfdist_transition:\n",
    "    for tag2 in cfdist_transition:\n",
    "        cfdist_transition[tag1][tag2] += 1\n",
    "\n",
    "            \n",
    "#to check if now cfdist contains all possible transitions we can do the following check\n",
    "#len(np.unique([len(cfdist_transition[key]) for key in cfdist_transition])) == 1\n",
    "#if TRUE, then all starting points can reach the same number of targets\n",
    "\n",
    "#normalizing inside each class\n",
    "for origin in cfdist_transition.keys():\n",
    "    local_count = sum([cfdist_transition[origin][i] for i in cfdist_transition[origin]])\n",
    "    for destination in cfdist_transition[origin].keys():\n",
    "        cfdist_transition[origin][destination] /= local_count\n",
    "\n",
    "a = cfdist_transition\n",
    "\n",
    "#checking that they sum ~1\n",
    "#for i in a:\n",
    "#    print(sum([a[i][j] for j in a[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finally, let's get the emission probabilities\n",
    "#similar structure than for the transitions\n",
    "def tagged_words_condit_counter(read_corpus):\n",
    "    #counts emission probabilities in the given corpus and returns them in the cfdist form\n",
    "    cfdist = nltk.probability.ConditionalFreqDist()\n",
    "    unique_words = []\n",
    "    for tagged_word in read_corpus.tagged_words():\n",
    "        tag = tagged_word[1]\n",
    "        word = tagged_word[0].lower()\n",
    "        unique_words.append(word)\n",
    "        #adding to a list the lower case words to produce a unique_words list after lower-casing\n",
    "        cfdist[tag][word] += 1\n",
    "    return cfdist, set(unique_words)\n",
    "\n",
    "cfdist_emission, unique_words = tagged_words_condit_counter(corp_train)\n",
    "\n",
    "#smoothing with k=1\n",
    "#added counts in this case is size of the vocab * number of tags\n",
    "\n",
    "for tag in cfdist_emission:\n",
    "    for word in unique_words:\n",
    "        cfdist_emission[tag][word] += 1\n",
    "        \n",
    "        \n",
    "\n",
    "#after smoothing all cfdist_emission[key] have the length of the whole vocab\n",
    "#added_counts_emissions = np.sqrt(added_counts) * len(cfdist_emission['IN'])\n",
    "            \n",
    "#to check if now cfdist_emission contains all possible emissions we can do the following check\n",
    "#len(np.unique([len(cfdist_emission[key]) for key in cfdist_emission])) == 1\n",
    "#if TRUE, then all states can generate all possible observations\n",
    "\n",
    "#normalizing\n",
    "#total_emissions = total_words + added_counts_emissions\n",
    "for tag in cfdist_emission.keys():\n",
    "    local_count = sum([cfdist_emission[tag][i] for i in cfdist_emission[tag]])\n",
    "    for word in cfdist_emission[tag].keys():\n",
    "        cfdist_emission[tag][word] /= local_count\n",
    "\n",
    "b = cfdist_emission\n",
    "\n",
    "#checking that they sum ~1\n",
    "#for i in a:\n",
    "#    print(sum([a[i][j] for j in a[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting a list of the states\n",
    "states = [i for i in cfdist_transition]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['PRP', 'VBP', 'VBG', 'TO', 'VB', 'IN', 'DT', 'NN', '.'],\n",
       " 7.08826171341732e-26)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = ['I', 'am', 'going', 'to', 'Berlin', 'for', 'the', 'weekend', '.']\n",
    "viterbi(obs, states, pi, a, b )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['PRP', 'VBP', 'VBN', 'TO', 'VB', 'IN', 'DT', 'NN', '.'],\n",
       " 1.5709593674225556e-22)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#trying with an invented \"word\"\n",
    "obs = ['I', 'am', 'kjabsdv', 'to', 'Berlin', 'for', 'the', 'weekend', '.']\n",
    "viterbi(obs, states, pi, a, b )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Explain which smoothing procedures you used and any observations.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTES\n",
    "* For the word counts we set all words to lower case. I am not sure how much that could have affected the tags. Woods last name would now be counted same as woods plural noun. Even with this, I believe more harm would be done if we count as separate nouns their capitalized and lowered versions.\n",
    "\n",
    "* How to handle if first observation is unk? For now, we choose randomly an initial state. \n",
    "* Should't we also set the a_ij to 1 whenever we encounter obs[j] = unk? Same logic as the hint for the emission case, since we don't know to which tag it belongs we don't have transition coef a_ij.\n",
    "\n",
    "* Didn't have time to implement other smoothing methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Evaluation [34 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Once you have trained a model, evaluate it on the unseen data from the test set. Run the Viterbi algorithm with each of your models, and output a tagged corpus in the two-column CoNLL format (*.tt). Use the provided evaluation script `tagging_eval.py`, which you can run on a gold annotated file and your own tagging results.<br><br>\n",
    "Run it on the output of your tagger and the evaluation set and report your results. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO\n",
    "* eval result\n",
    "* eval with other k-smoothings\n",
    "* eval with smoothing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# too inefficient!\n",
    "# How to make it better?\n",
    "def corpus_viterbi(read_corpus):\n",
    "    global states\n",
    "    global pi\n",
    "    global a\n",
    "    global b\n",
    "    tagged_sentences = []\n",
    "    for sentence in read_corpus.sents():\n",
    "        tagged_sentences.append(viterbi(sentence, states, pi, a, b)[0])\n",
    "    return tagged_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rodri\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:44: RuntimeWarning: divide by zero encountered in log\n"
     ]
    }
   ],
   "source": [
    "tagging_test = corpus_viterbi(corp_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_tags(fname_gold, fname_model):\n",
    "    n = 0\n",
    "    pr_counts = {}\n",
    "    with open(fname_gold) as gold, open(fname_model) as model:\n",
    "        for line_gold, line_model in zip(gold, model):\n",
    "            line_gold = line_gold.rstrip()\n",
    "            line_model = line_model.rstrip()\n",
    "            n += 1\n",
    "            if len(line_gold) == 0:\n",
    "                continue\n",
    "\n",
    "            word_gold, tag_gold = line_gold.split(\"\\t\")\n",
    "            word_model, tag_model = line_model.split(\"\\t\")\n",
    "\n",
    "            # word forms should match in gold and system file\n",
    "            if word_model != word_gold:\n",
    "                raise ValueError(\"\\nError in line {}: Words in gold and model files are different. Either your gold \"\n",
    "                                 \"file is broken or your model file was generated incorrectly.\".format(n))\n",
    "\n",
    "            if tag_gold not in pr_counts:\n",
    "                pr_counts[tag_gold] = {\"correct\": 0, \"total_model\": 0, \"total_gold\": 0}\n",
    "            if tag_model not in pr_counts:\n",
    "                pr_counts[tag_model] = {\"correct\": 0, \"total_model\": 0, \"total_gold\": 0}\n",
    "            pr_counts[tag_gold][\"total_gold\"] += 1\n",
    "            pr_counts[tag_model][\"total_model\"] += 1\n",
    "            if tag_gold == tag_model:\n",
    "                pr_counts[tag_model][\"correct\"] += 1\n",
    "\n",
    "        print(\"\\nComparing gold file {} with model output file {}...\".format(fname_gold, fname_model))\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        print(\"\\nTAG / Precision / Recall / F1\\n\")\n",
    "        for tag, counts in pr_counts.items():\n",
    "            correct += counts[\"correct\"]\n",
    "            total += counts[\"total_gold\"]\n",
    "            if 0 in counts.values():\n",
    "                print(\"Tag {} was never correctly assigned. One of precision, recall or F1 is undefined!\".format(tag))\n",
    "            else:\n",
    "                precision = counts[\"correct\"] / counts[\"total_model\"]\n",
    "                recall = counts[\"correct\"] / counts[\"total_gold\"]\n",
    "                f1 = (2 * precision * recall) / (precision + recall)\n",
    "                print(\"{:5s}: {:4f} / {:4f} / {:4f}\".format(tag, precision, recall, f1))\n",
    "\n",
    "        print(\"\\nOverall Accuracy: {:5f}\".format(correct / total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Discuss the results of the different tagger versions.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your discussion goes here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Credit [10 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "The task is challenging as it stands. However, feel free to go further for extra credit, e.g. by doing one of the following: implement better unknown word handling, use a trigram tagger, plot a learning curve for your tagger (accuracy as a function of training data size), plot a speed vs. sentence length curve.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# space for your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your discussion*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "ctrl-q"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANLP 2019 - Assignment 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Rodrigo Lopez Portillo Alcocer, * (enter your name/student id number here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">Due: Wednesday, November 20</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "**NOTE**<br><br>\n",
    "\n",
    "Please first fill in your name and id number at the top of the assignment, and **rename** the assignment file to **yourlastname-anlp-1.ipynb**<br><br>\n",
    "Problems and questions are given in blue boxes like this one. All grey and white boxes must be filled by you (they either require code or a (brief!) discussion). <br><br>\n",
    "Please hand in your assignment by the deadline via Moodle upload (we will provide a link). In case of questions, you can contact David, Bethany or Patrick via email, or via the Moodle forum (preferred).<br><br>\n",
    "<b>For this assignment, do NOT use any external packages (NLTK or any others) EXCEPT where specified.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "In this assignment, you will implement and work with a Naive Bayes classifier. (Note that for this exercise, you don't need to represent the input as a vector necessarily. You can directly look at the presence of words, and look up the class conditional likelihood.)\n",
    "<br>\n",
    "<br>\n",
    "We will use a Twitter dataset classified into \"hate speech\" and \"non hate speech\" (in our data, we have called these classes \"offensive\" and \"nonoffensive\" to avoid the charged and inaccurate term \"hate speech\"). First, load the data (we have provided the function for this):\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: 12896\n",
      "Test data: 3250\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import simplejson as json\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "def read_hate_tweets (annofile, jsonfile):\n",
    "    \"\"\"Reads in hate speech data.\"\"\"\n",
    "    all_data = {}\n",
    "    annos = {}\n",
    "    with open(annofile) as csvfile:\n",
    "        csvreader = csv.reader(csvfile, delimiter=',')\n",
    "        for row in csvreader:\n",
    "            if row[0] in annos:\n",
    "                # if duplicate with different rating, remove!\n",
    "                if row[1] != annos[row[0]]:\n",
    "                    del(annos[row[0]])\n",
    "            else:\n",
    "                annos[row[0]] = row[1]\n",
    "\n",
    "    tknzr = TweetTokenizer()\n",
    "                \n",
    "    with open(jsonfile) as jsonfile:\n",
    "        for line in jsonfile:\n",
    "            twtjson = json.loads(line)\n",
    "            twt_id = twtjson['id_str']\n",
    "            if twt_id in annos:\n",
    "                all_data[twt_id] = {}\n",
    "                all_data[twt_id]['offensive'] = \"nonoffensive\" if annos[twt_id] == 'none' else \"offensive\"\n",
    "                all_data[twt_id]['text_tok'] = tknzr.tokenize(twtjson['text'])\n",
    "\n",
    "    # split training and test data:\n",
    "    all_data_sorted = sorted(all_data.items())\n",
    "    items = [(i[1]['text_tok'],i[1]['offensive']) for i in all_data_sorted]\n",
    "    splititem = len(all_data)-3250\n",
    "    train_dt = items[:splititem]\n",
    "    test_dt = items[splititem:]\n",
    "    print('Training data:',len(train_dt))\n",
    "    print('Test data:',len(test_dt))\n",
    "\n",
    "    return(train_dt,test_dt)\n",
    "\n",
    "TWEETS_ANNO = 'NAACL_SRW_2016.csv'\n",
    "TWEETS_TEXT = 'NAACL_SRW_2016_tweets.json'\n",
    "\n",
    "(train_data,test_data) = read_hate_tweets(TWEETS_ANNO,TWEETS_TEXT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Each item in our data consists of a tuple of the tweet text and its label (represented as a string). The tweet text has been tokenized and is represented as a list of words. We can look at an example item:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nonoffensive'"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[400][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "The first thing you're being asked to do is to provide evaluation functions for a classifier and a given labelled test set. Assume that the classifier has a `predict()` function that takes an item in the form of a list as above and predicts a class for that item. Write evaluation functions to compute the `accuracy` and `f_1` score for such a classifier.\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(classifier, data):\n",
    "    \"\"\"Computes the accuracy of a classifier on reference data.\n",
    "\n",
    "    Args:\n",
    "        classifier: A classifier.\n",
    "        data: Reference data.\n",
    "\n",
    "    Returns:\n",
    "        The accuracy of the classifier on the test data, a float.\n",
    "    \"\"\"\n",
    "    # TODO: Replace the following line with your own code\n",
    "    # We will assume that the labelled test data has the same structure as the data loaded above (x[0] is the text and x[1] the label)\n",
    "    # first we will create a binary list storing if the prediction was correct or not\n",
    "    predict_reviewed = [classifier.predict(text[0]) == text[1] for text in data]\n",
    "    # accuracy = number of correct predictions / total number of predictions\n",
    "    return np.sum(predict_reviewed) / len(predict_reviewed)\n",
    "\n",
    "def f_1(classifier, data):\n",
    "    \"\"\"Computes the F_1-score of a classifier on reference data.\n",
    "\n",
    "    Args:\n",
    "        classifier: A classifier.\n",
    "        data: Reference data.\n",
    "\n",
    "    Returns:\n",
    "        The F_1-score of the classifier on the test data, a float.\n",
    "    \"\"\"\n",
    "    # TODO: Replace the following line with your own code\n",
    "    # Same assumption as before and assuming the labels are 1 and 0 for positive and negative\n",
    "    # In order to compute the F1 score we will calculate first both precision and recall\n",
    "    true_pos = np.sum([classifier.predict(text[0]) =='offensive' and text[1]=='offensive' for text in data])\n",
    "    false_pos = np.sum([classifier.predict(text[0])=='offensive' and text[1]=='nonoffensive' for text in data])\n",
    "    false_neg = np.sum([classifier.predict(text[0])=='nonoffensive' and text[1]=='offensive' for text in data])\n",
    "    precision = true_pos/(true_pos + false_pos)\n",
    "    recall = true_pos/(true_pos + false_neg)\n",
    "    return 2*precision*recall/(precision+recall)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Next, implement the Naive Bayes classifier from scratch using the code skeleton below and the definitions from class.<br><br>\n",
    "Some requirements and notes for implementation:\n",
    "<ul>\n",
    "<li>You should allow for an arbitrary number of classes (in particular, you should not hard code the two classes needed for the given dataset). \n",
    "<li>The vocabulary of your classifier should be created dynamically from the training data. (The vocabulary is the set of all words that occur in the training data.).\n",
    "<li>Use additive smoothing with a provided parameter k. \n",
    "<li>You may encounter unknown words at test time. Since we're not allowed to \"peek\" into the test set, we will implement the following simple treatment: We will assume that we don't know anything about unknown words and that in particular, their presence does not tell us anything about which class a document should be assigned to. Therefore, we will not include them in the calculation of the (log) probabilities during prediction, under the assumption that their probability does not differ hugely between the different classes (probably not a correct assumption, but the best we can do at this point). Since we don't need correct probabilities but only most likely classes, just ignore unknown words during prediction.\n",
    "<li> Use log probabilities in order to avoid underflow.\n",
    "</ul>\n",
    "\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialises a new classifier.\"\"\"\n",
    "        # TODO: Replace the following line with your own code\n",
    "        self.voc = dict()\n",
    "        self.counts = list()\n",
    "        self.totals = list()\n",
    "        pass\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"Predicts the class for a document.\n",
    "\n",
    "        Args:\n",
    "            x: A document, represented as a list of words.\n",
    "\n",
    "        Returns:\n",
    "            The predicted class, represented as a string.\n",
    "        \"\"\"\n",
    "        # TODO: Replace the following line with your own code\n",
    "        x = [word.lower() for word in x]\n",
    "        classes = list(self.voc.keys()) #get list of all classes\n",
    "        probs = np.ones([len(classes), len(x)+1]) # initialize matrix with probabilities\n",
    "        #the +1 is to add a column with the probabilities of each category from self.totals\n",
    "        for i in range(len(classes)): #scanning all classes\n",
    "            for j in range(len(x)): #scanning all words\n",
    "                if x[j] in self.voc[classes[i]].keys(): #if the word appears in the class\n",
    "                    probs[i][j] = self.voc[classes[i]][x[j]] \n",
    "                else: #if it doesn't we'll assign a value of 1\n",
    "                    probs[i][j] = 1\n",
    "        for i in range(len(classes)):\n",
    "            probs[i][-1] = self.totals[i]\n",
    "        results = np.array([1, len(classes)]) #here we will store the computed probabilities\n",
    "        results = [np.prod(probs[i]) for i in range(len(probs))]\n",
    "        return classes[np.argmax(results)]\n",
    "    \n",
    "    def CountFrequency(self, my_list): \n",
    "        freq = {} \n",
    "        for item in my_list: \n",
    "            if (item in freq): \n",
    "                freq[item] += 1\n",
    "            else: \n",
    "                freq[item] = 1\n",
    "        return freq\n",
    "\n",
    "        \n",
    "    @classmethod\n",
    "    def train(cls, data, k=1):\n",
    "        \"\"\"Train a new classifier on training data using maximum\n",
    "        likelihood estimation and additive smoothing.\n",
    "\n",
    "        Args:\n",
    "            cls: The Python class representing the classifier.\n",
    "            data: Training data.\n",
    "            k: The smoothing constant.\n",
    "\n",
    "        Returns:\n",
    "            A trained classifier, an instance of `cls`.\n",
    "        \"\"\"\n",
    "        self = cls()\n",
    "        #we will create a dictionary with the word counts for each class\n",
    "        #voc = dict()  #voc by classes\n",
    "        #counts = list() #ith element will contain the total number of words in ith class\n",
    "        for i in range(len(data)):\n",
    "            temp = dict() #we will store here the frequency counts for the current line\n",
    "            v = data[i] #vector with line tokens\n",
    "            c = v[1] #class\n",
    "            if (c not in self.voc): #if the class has never been seen before add it to the dictionary\n",
    "                self.voc[c] = dict()\n",
    "            new_words = [x.lower() for x in v[0]] #making sure all our new words are in lower case to avoid repetitions\n",
    "            temp = CountFrequency(new_words) # add the freq counts for the new vocab to the temporary dictionary\n",
    "            self.voc[c] = Counter(self.voc[c]) + Counter(temp)#updating our current dictionary\n",
    "        \n",
    "        #counting probability of each class based on number of words\n",
    "        self.totals = [np.sum(list(self.voc[key].values())) for key in self.voc.keys()]\n",
    "        total = np.sum(self.totals)\n",
    "        self.totals = [self.totals[i]/total for i in range(len(self.totals))]\n",
    "        \n",
    "        # k-smoothing\n",
    "        for cla in self.voc:\n",
    "            for word in self.voc[cla]:\n",
    "                self.voc[cla][word] += k\n",
    "\n",
    "        # to get the probabilities now we only have to count all words in each class and normalize\n",
    "        i=0\n",
    "        for cla in self.voc:\n",
    "            t = 0\n",
    "            for word in self.voc[cla]:\n",
    "                t += self.voc[cla][word]\n",
    "            self.counts.append(t)\n",
    "            i+=1\n",
    "\n",
    "        # now we normalize our values\n",
    "        j = 0\n",
    "        for cla in self.voc:\n",
    "            for word in self.voc[cla]:\n",
    "                self.voc[cla][word] /= self.counts[j]\n",
    "            j +=1\n",
    "        \n",
    "        # and we apply log likelihood\n",
    "        # probably all this could fit in the same for loop but no time to re structure rn\n",
    "        for cla in self.voc:\n",
    "            for word in self.voc[cla]:\n",
    "                self.voc[cla][word] = np.abs(np.log(self.voc[cla][word])) \n",
    "                \n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Evaluate your classifier by training and testing it on the given data. Vary the smoothing parameter k. What happens when you decrease k? Plot a graph of the accuracy and/or f-score given different values of k. Discuss your findings.<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb0 = NaiveBayes.train(train_data, 0)\n",
    "#nb1 = NaiveBayes.train(train_data, 1)\n",
    "nb5 = NaiveBayes.train(train_data, 5)\n",
    "#nb200 = NaiveBayes.train(train_data, 200)\n",
    "acc_0 = accuracy(nb0, test_data)\n",
    "#acc_1 = accuracy(nb1, test_data)\n",
    "acc_5 = accuracy(nb5, test_data)\n",
    "#acc_200 = accuracy(nb200, test_data)\n",
    "f1_0 = f_1(nb0,test_data)\n",
    "#f1_1 = f_1(nb1,test_data)\n",
    "f1_5 = f_1(nb5,test_data)\n",
    "#f1_200 = f_1(nb200,test_data)\n",
    "#print(\"Accuracy for k=0: \",acc_0)\n",
    "#print(\"Accuracy for k=1: \",acc_1)\n",
    "#print(\"Accuracy for k=5: \",acc_5)\n",
    "#print(\"Accuracy for k=200: \",acc_200)\n",
    "#print(\"F_1 for k=0: \", f1_0)\n",
    "#print(\"F_1 for k=1: \", f1_1)\n",
    "#print(\"F_1 for k=5: \", f1_5)\n",
    "#print(\"F_1 for k=200: \", f1_200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now that everything seems to work out we can compute several f_1 scores to compare them\n",
    "ks = np.arange(0, 20, 5)\n",
    "ks = np.append(ks,25)\n",
    "ks = np.append(ks,40)\n",
    "ks = np.append(ks,60)\n",
    "ks = np.append(ks,90)\n",
    "ks = np.append(ks,120)\n",
    "k_values = []\n",
    "NB = [NaiveBayes.train(train_data, k) for k in ks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_acc = [accuracy(NB[i], test_data) for i in range(len(NB))]\n",
    "scores_f1 = [f_1(NB[i], test_data) for i in range(len(NB))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'F1 Score Variation')"
      ]
     },
     "execution_count": 547,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#now we can compute several accuracy values to compare them\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "#accuracies\n",
    "plt.subplot(121)\n",
    "plt.plot(ks, scores_acc, 'bo')\n",
    "plt.title(\"Accuracy Variation\")\n",
    "\n",
    "\n",
    "#f1 score\n",
    "plt.subplot(122)\n",
    "plt.plot(ks, scores_f1, 'bo')\n",
    "plt.title(\"F1 Score Variation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not the nicest plot but it would seem that the higher our k value is, the higher our f_1 score. At least up to a certain k this might be true, but after k passes a certain threshold the smoothing will be too much for and the original counts will be irrelevant. This would in turn lead to a meaningless analysis. We observe very similar result behaviour for the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "We mentioned that the Naive Bayes classifier can be used with many different feature types. Try to improve on the basic bag of words model by changing the feature list of your model. Implement at least two variants. For each, explain your motivation for this feature set, and test the classifier with the given data. Briefly discuss your results!<br><br> \n",
    "Ideas for feature sets that were mentioned in class include:\n",
    "<ul>\n",
    "<li>removing stop words or frequent words\n",
    "<li>stemming or lemmatizing (you can use NLTK or spacy.io for basic NLP operations on the texts)\n",
    "<li>introducing part of speech tags as features (how?)\n",
    "<li>bigrams\n",
    "</ul>\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The first feature we will try to implement is to delete from the word counts the 5% most frequent words because they are likely\n",
    "# to be irrelevant to our analysis.\n",
    "class NaiveBayes_feature1(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialises a new classifier.\"\"\"\n",
    "        # TODO: Replace the following line with your own code\n",
    "        self.voc = dict()\n",
    "        self.counts = list()\n",
    "        self.freq = list()\n",
    "        self.totals = list()\n",
    "        pass\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"Predicts the class for a document.\n",
    "\n",
    "        Args:\n",
    "            x: A document, represented as a list of words.\n",
    "\n",
    "        Returns:\n",
    "            The predicted class, represented as a string.\n",
    "        \"\"\"\n",
    "        # TODO: Replace the following line with your own code\n",
    "        x = [word.lower() for word in x]\n",
    "        classes = list(self.voc.keys()) #get list of all classes\n",
    "        probs = np.ones([len(classes), len(x)]) # initialize matrix with probabilities\n",
    "        for i in range(len(classes)): #scanning all classes\n",
    "            for j in range(len(x)): #scanning all words\n",
    "                if x[j] in self.voc[classes[i]].keys(): #if the word appears in the class\n",
    "                    probs[i][j] = self.voc[classes[i]][x[j]] \n",
    "                else: #if it doesn't we'll assign a value of 1\n",
    "                    probs[i][j] = 1\n",
    "        results = np.array([1, len(classes)]) #here we will store the computed probabilities\n",
    "        results = [np.prod(probs[i]) for i in range(len(probs))]\n",
    "        return classes[np.argmax(results)]\n",
    "    \n",
    "    def CountFrequency(self, my_list): \n",
    "        freq = {} \n",
    "        for item in my_list: \n",
    "            if (item in freq): \n",
    "                freq[item] += 1\n",
    "            else: \n",
    "                freq[item] = 1\n",
    "        return freq\n",
    "\n",
    "        \n",
    "    @classmethod\n",
    "    def train(cls, data, k=1):\n",
    "        \"\"\"Train a new classifier on training data using maximum\n",
    "        likelihood estimation and additive smoothing.\n",
    "\n",
    "        Args:\n",
    "            cls: The Python class representing the classifier.\n",
    "            data: Training data.\n",
    "            k: The smoothing constant.\n",
    "\n",
    "        Returns:\n",
    "            A trained classifier, an instance of `cls`.\n",
    "        \"\"\"\n",
    "        self = cls()\n",
    "        #we will create a dictionary with the word counts for each class\n",
    "        #voc = dict()  #voc by classes\n",
    "        #counts = list() #ith element will contain the total number of words in ith class\n",
    "        for i in range(len(data)):\n",
    "            temp = dict() #we will store here the frequency counts for the current line\n",
    "            v = data[i] #vector with line tokens\n",
    "            c = v[1] #class\n",
    "            if (c not in self.voc): #if the class has never been seen before add it to the dictionary\n",
    "                self.voc[c] = dict()\n",
    "            new_words = [x.lower() for x in v[0]] #making sure all our new words are in lower case to avoid repetitions\n",
    "            temp = CountFrequency(new_words) # add the freq counts for the new vocab to the temporary dictionary\n",
    "            self.voc[c] = Counter(self.voc[c]) + Counter(temp)#updating our current dictionary\n",
    "            \n",
    "        #counting probability of each class based on number of words\n",
    "        self.totals = [np.sum(list(self.voc[key].values())) for key in self.voc.keys()]\n",
    "        total = np.sum(self.totals)\n",
    "        self.totals = [self.totals[i]/total for i in range(len(self.totals))]\n",
    "        \n",
    "        #adding the mentioned feature\n",
    "        for cla in self.voc:\n",
    "            self.freq = [self.voc[cla][i] for i in self.voc[cla].keys()]\n",
    "            percentile = np.percentile(self.freq, 95) #getting the count of the 95% percentile\n",
    "            for word in self.voc[cla]:\n",
    "                if self.voc[cla][word] >= percentile: # if word has a higher count than the 95% percentile we will ignore it\n",
    "                    #del self.voc[cla][word] doesn t work because dictionary changes size during loop\n",
    "                    self.voc[cla][word] = 1\n",
    "\n",
    "        # k-smoothing\n",
    "        for cla in self.voc:\n",
    "            for word in self.voc[cla]:\n",
    "                self.voc[cla][word] += k\n",
    "\n",
    "        # to get the probabilities now we only have to count all words in each class and normalize\n",
    "        i=0\n",
    "        for cla in self.voc:\n",
    "            t = 0\n",
    "            for word in self.voc[cla]:\n",
    "                t += self.voc[cla][word]\n",
    "            self.counts.append(t)\n",
    "            i+=1\n",
    "\n",
    "        # now we normalize our values\n",
    "        j = 0\n",
    "        for cla in self.voc:\n",
    "            for word in self.voc[cla]:\n",
    "                self.voc[cla][word] /= self.counts[j]\n",
    "            j +=1\n",
    "        \n",
    "        # and we apply log likelihood\n",
    "        # probably all this could fit in the same for loop but no time to re structure rn\n",
    "        for cla in self.voc:\n",
    "            for word in self.voc[cla]:\n",
    "                self.voc[cla][word] = np.abs(np.log(self.voc[cla][word])) \n",
    "                \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.72 0.7996923076923077\n",
      "0.7550769230769231 0.807076923076923\n",
      "0.23011844331641285 0.2627406568516421\n",
      "0.2316602316602317 0.26148409893992935\n"
     ]
    }
   ],
   "source": [
    "#feature1_0 = NaiveBayes_feature1.train(train_data, 0)\n",
    "#feature1_5 = NaiveBayes_feature1.train(train_data, 5)\n",
    "acc_f1_0 = accuracy(feature1_0, test_data)\n",
    "acc_f1_5 = accuracy(feature1_5, test_data)\n",
    "f1_f1_0 = f_1(feature1_0,test_data)\n",
    "f1_f1_5 = f_1(feature1_5,test_data)\n",
    "print(acc_0,acc_f1_0)\n",
    "print(acc_5,acc_f1_5)\n",
    "print(f1_0,f1_f1_0)\n",
    "print(f1_5,f1_f1_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see improvements for both accuracy and f_1 score for the cases k=0 and k=5. Further testing is required to make sure that this feature is giving legit results and it is not coincidence. Testing with different percentiles and different k values would be even better.\n",
    "\n",
    "Our next hypothesis is that taking into account the information given by # and @ is misleading when it comes to classifying the sentence. I am not really sure how they word since I don't have social media :P but let's try to implement it and see the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes_feature2(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialises a new classifier.\"\"\"\n",
    "        # TODO: Replace the following line with your own code\n",
    "        self.voc = dict()\n",
    "        self.counts = list()\n",
    "        self.totals = list()\n",
    "        pass\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"Predicts the class for a document.\n",
    "\n",
    "        Args:\n",
    "            x: A document, represented as a list of words.\n",
    "\n",
    "        Returns:\n",
    "            The predicted class, represented as a string.\n",
    "        \"\"\"\n",
    "        # TODO: Replace the following line with your own code\n",
    "        x = [word.lower() for word in x]\n",
    "        classes = list(self.voc.keys()) #get list of all classes\n",
    "        probs = np.ones([len(classes), len(x)]) # initialize matrix with probabilities\n",
    "        for i in range(len(classes)): #scanning all classes\n",
    "            for j in range(len(x)): #scanning all words\n",
    "                if x[j] in self.voc[classes[i]].keys(): #if the word appears in the class\n",
    "                    probs[i][j] = self.voc[classes[i]][x[j]] \n",
    "                else: #if it doesn't we'll assign a value of 1\n",
    "                    probs[i][j] = 1\n",
    "        results = np.array([1, len(classes)]) #here we will store the computed probabilities\n",
    "        results = [np.prod(probs[i]) for i in range(len(probs))]\n",
    "        return classes[np.argmax(results)]\n",
    "    \n",
    "    def CountFrequency(self, my_list): \n",
    "        freq = {} \n",
    "        for item in my_list: \n",
    "            if (item in freq): \n",
    "                freq[item] += 1\n",
    "            else: \n",
    "                freq[item] = 1\n",
    "        return freq\n",
    "\n",
    "        \n",
    "    @classmethod\n",
    "    def train(cls, data, k=1):\n",
    "        \"\"\"Train a new classifier on training data using maximum\n",
    "        likelihood estimation and additive smoothing.\n",
    "\n",
    "        Args:\n",
    "            cls: The Python class representing the classifier.\n",
    "            data: Training data.\n",
    "            k: The smoothing constant.\n",
    "\n",
    "        Returns:\n",
    "            A trained classifier, an instance of `cls`.\n",
    "        \"\"\"\n",
    "        self = cls()\n",
    "        #we will create a dictionary with the word counts for each class\n",
    "        #voc = dict()  #voc by classes\n",
    "        #counts = list() #ith element will contain the total number of words in ith class\n",
    "        for i in range(len(data)):\n",
    "            temp = dict() #we will store here the frequency counts for the current line\n",
    "            v = data[i] #vector with line tokens\n",
    "            c = v[1] #class\n",
    "            if (c not in self.voc): #if the class has never been seen before add it to the dictionary\n",
    "                self.voc[c] = dict()\n",
    "            new_words = [x.lower() for x in v[0]] #making sure all our new words are in lower case to avoid repetitions\n",
    "            temp = CountFrequency(new_words) # add the freq counts for the new vocab to the temporary dictionary\n",
    "            self.voc[c] = Counter(self.voc[c]) + Counter(temp)#updating our current dictionary\n",
    "        \n",
    "        #counting probability of each class based on number of words\n",
    "        self.totals = [np.sum(list(self.voc[key].values())) for key in self.voc.keys()]\n",
    "        total = np.sum(self.totals)\n",
    "        self.totals = [self.totals[i]/total for i in range(len(self.totals))]\n",
    "            \n",
    "        #adding the mentioned feature\n",
    "        for cla in self.voc:\n",
    "            for word in self.voc[cla]:\n",
    "                if word.startswith('#') or word.startswith('@'):\n",
    "                    self.voc[cla][word] = 1\n",
    "\n",
    "        # k-smoothing\n",
    "        for cla in self.voc:\n",
    "            for word in self.voc[cla]:\n",
    "                self.voc[cla][word] += k\n",
    "\n",
    "        # to get the probabilities now we only have to count all words in each class and normalize\n",
    "        i=0\n",
    "        for cla in self.voc:\n",
    "            t = 0\n",
    "            for word in self.voc[cla]:\n",
    "                t += self.voc[cla][word]\n",
    "            self.counts.append(t)\n",
    "            i+=1\n",
    "\n",
    "        # now we normalize our values\n",
    "        j = 0\n",
    "        for cla in self.voc:\n",
    "            for word in self.voc[cla]:\n",
    "                self.voc[cla][word] /= self.counts[j]\n",
    "            j +=1\n",
    "        \n",
    "        # and we apply log likelihood\n",
    "        # probably all this could fit in the same for loop but no time to re structure rn\n",
    "        for cla in self.voc:\n",
    "            for word in self.voc[cla]:\n",
    "                self.voc[cla][word] = np.abs(np.log(self.voc[cla][word])) \n",
    "                \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.72 0.608\n",
      "0.7550769230769231 0.6258461538461538\n",
      "0.23011844331641285 0.23067632850241543\n",
      "0.2316602316602317 0.23037974683544304\n"
     ]
    }
   ],
   "source": [
    "feature2_0 = NaiveBayes_feature2.train(train_data, 0)\n",
    "feature2_5 = NaiveBayes_feature2.train(train_data, 5)\n",
    "acc_f2_0 = accuracy(feature2_0, test_data)\n",
    "acc_f2_5 = accuracy(feature2_5, test_data)\n",
    "f1_f2_0 = f_1(feature2_0,test_data)\n",
    "f1_f2_5 = f_1(feature2_5,test_data)\n",
    "print(acc_0,acc_f2_0)\n",
    "print(acc_5,acc_f2_5)\n",
    "print(f1_0,f1_f2_0)\n",
    "print(f1_5,f1_f2_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying this feature reduced our accuracy for the tested k values (0 and 5). The f1 score was not affected too much by it. The training times where around 50% longer, so in general I wouldn't keep these feature in the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Logistic Regression/MaxEnt Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "As a bonus, implement a MaxEnt classifier using the definitions given in class and gradient ascent. For this, you will have to use a matrix representation for your data to keep track of each feature's weights per class, which you can implement using the `numpy` package. <br><br> \n",
    "Start by implementing a function `featurize()` that converts the (training or testing) data into a matrix format. This function should return a pair of NumPy matrices ùëø, ùíÄ, where ùëø is an ùëÅ √ó ùêπ matrix (ùëÅ: number of data instances, ùêπ: number of features), and where ùíÄ is an ùëÅ √ó 2 matrix whose rows have one of two forms:<br><br>\n",
    "[1, 0] if the gold-standard annotation class for the corresponding tweet is ‚Äòoffensive‚Äô, or <br><br>\n",
    "[0, 1] if the gold-standard class for the corresponding document is ‚Äònonoffensive‚Äô<br><br>\n",
    "This kind of representation is known as a one-hot encoding. You can read the first vector as saying that ‚Äòthere is a 100% chance that the instance belongs to the ‚Äúoffensive‚Äù class and a 0% chance that it belongs to the ‚Äúnonoffensive‚Äù class‚Äô, and similarly for the second vector. Note that these are the two extreme cases for the conditional probability distribution P(k|x) for class k and feature vector x.<br><br>\n",
    "To implement the `featurize()` function, you will need to assign to each word in the training set a unique integer index which will identify that component of the feature vector which is 1 if the corresponding word is present in the document, and 0 otherwise. This index is built by the helper function `build_w2i()`.<br><br>\n",
    "Your next task is to complete the implementation of the `MaxEnt` class. The methods `p()` and `predict()` yield the probability of a class given an item, and the best class for the item, respectively. They can be implemented using appropriate NumPy matrix operations and the provided `softmax()` function. Note that you should set up both methods to take a whole matrix of input vectors as input, not just a single vector.<br><br>\n",
    "The training procedure is implemented in the (class) method `train()`, using iterative optimization. Typically, we shuffle the training data and split them into mini-batches (e.g, 100 items), then update the weights after each minibatch. This is done for `max_iter` number of iterations, or \"epochs\". Each epoch iterates over the training data set once.<br><br>\n",
    "Implement the missing methods using l_2 regularization with parameter C=0.1\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MaxEnt:\n",
    "    def __init__(self, eta=0.01, num_iter=30):\n",
    "        self.eta = eta\n",
    "        self.num_iter = num_iter\n",
    "    \n",
    "    def softmax(self, inputs):\n",
    "        \"\"\"\n",
    "        Calculate the softmax for the give inputs (array)\n",
    "        :param inputs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return np.exp(inputs) / float(sum(np.exp(inputs)))\n",
    "    \n",
    "    def train(self, X, Y):\n",
    "\n",
    "        # weights initialization\n",
    "        self.weights = np.zeros(X.shape[1])\n",
    "        \n",
    "        for i in range(self.num_iter):\n",
    "            # TODO: Fill in iterative updating of weights\n",
    "        return\n",
    "    \n",
    "    def p(self, X): \n",
    "        # TODO: Fill in (log) probability prediction\n",
    "        return 0.0\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # TODO: Replace next line with prediction of best class\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Test your implementation using 10 iterations, default learning rate eta, and l_2 regularization with parameter C=0.1.\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "ctrl-q"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
